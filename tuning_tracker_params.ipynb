{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freida20git/bird-detection-tracking/blob/main/tuning_tracker_params.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploring different hyperparameters of deepSORT tracking algorithm**"
      ],
      "metadata": {
        "id": "t54Zjnv0JWa5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bzxkwNQKJ1HO"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llcnqXehMhHV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install deep_sort_realtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 'https://drive.google.com/uc?id=1x-A9WOyZtZrOqlgM-EXg5g4ek6YgmCYO'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tKuD1SyMqQi",
        "outputId": "6f045a60-c8cb-4341-ffd1-47c49dc6ffd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1x-A9WOyZtZrOqlgM-EXg5g4ek6YgmCYO\n",
            "To: /content/bestbirdsonly.pt\n",
            "100% 5.47M/5.47M [00:00<00:00, 27.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHg-KGE4G8ei",
        "outputId": "cacc5068-ace2-4286-f191-a7c46fb827cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1S9jEgd9m6O6O9srOapjyFG0tUU0IzfXM\n",
            "To: /content/birds-flying-in-blue-sky-preview.mp4\n",
            "100% 5.23M/5.23M [00:00<00:00, 30.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1S9jEgd9m6O6O9srOapjyFG0tUU0IzfXM\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "import cv2\n",
        "import json  # Added for JSON support\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "import json"
      ],
      "metadata": {
        "id": "FjYSsQ1PL-Xg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c3be4f-787c-4d57-c085-466c0ef222f5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input video and detection model\n",
        "video_path = '/content/birds-flying-in-blue-sky-preview.mp4'\n",
        "model_path = '/content/bestbirdsonly.pt'\n",
        "\n",
        "# DeepSORT parameter sets to test\n",
        "experiments = [\n",
        "    {\"max_cosine_distance\": 0.2, \"max_iou_distance\": 0.7, \"max_overlap\": 0.8},  # strict appearance\n",
        "    {\"max_cosine_distance\": 0.3, \"max_iou_distance\": 0.5, \"max_overlap\": 0.7},  # relaxed appearance\n",
        "    {\"max_cosine_distance\": 0.25, \"max_iou_distance\": 0.6, \"max_overlap\": 0.5}, # balanced\n",
        "    {\"max_cosine_distance\": 0.15, \"max_iou_distance\": 0.5, \"max_overlap\": 0.9}, # super strict appearance\n",
        "    {\"max_cosine_distance\": 0.35, \"max_iou_distance\": 0.4, \"max_overlap\": 0.6}, # feature noise tolerant\n",
        "    {\"max_cosine_distance\": 0.2, \"max_iou_distance\": 0.8, \"max_overlap\": 0.9},  # relaxed IoU\n",
        "    {\"max_cosine_distance\": 0.3, \"max_iou_distance\": 0.3, \"max_overlap\": 0.5},  # very tight matching\n",
        "    {\"max_cosine_distance\": 0.25, \"max_iou_distance\": 0.75, \"max_overlap\": 0.8},# fast movement tolerant\n",
        "    {\"max_cosine_distance\": 0.4, \"max_iou_distance\": 0.6, \"max_overlap\": 0.7},  # relaxed both\n",
        "    {\"max_cosine_distance\": 0.2, \"max_iou_distance\": 0.6, \"max_overlap\": 0.6},  # general-purpose\n",
        "]\n",
        "\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# Loop through each experiment\n",
        "for exp_num, params in enumerate(experiments):\n",
        "    print(f\"Running experiment {exp_num+1} with params: {params}\")\n",
        "\n",
        "    # Set up video capture\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get video properties\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Set up video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(f'output_hyp{exp_num+1}.mp4', fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    # Initialize tracker with params\n",
        "    tracker = DeepSort(\n",
        "        max_cosine_distance=params[\"max_cosine_distance\"],\n",
        "        max_iou_distance=params[\"max_iou_distance\"],\n",
        "        nms_max_overlap=params[\"max_overlap\"] )\n",
        "\n",
        "    frame_number = 0\n",
        "    all_annotations = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_number += 1\n",
        "        results = model(frame, conf=0.3, classes=0)\n",
        "\n",
        "        frame_annotations = {\n",
        "            \"frame_number\": frame_number,\n",
        "            \"objects\": []\n",
        "        }\n",
        "\n",
        "        detections = []\n",
        "        for *xyxy, conf, cls in results[0].boxes.data:\n",
        "            x1, y1, x2, y2 = map(int, xyxy)\n",
        "            detections.append([[x1, y1, x2 - x1, y2 - y1], conf.item(), int(cls.item())])\n",
        "\n",
        "        tracks = tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "        for track in tracks:\n",
        "            if not track.is_confirmed():\n",
        "                continue\n",
        "            track_id = track.track_id\n",
        "            ltrb = track.to_ltrb()\n",
        "            class_id = track.get_det_class()\n",
        "            x1, y1, x2, y2 = map(int, ltrb)\n",
        "            confidence = track.det_conf\n",
        "\n",
        "            if confidence is not None:\n",
        "                frame_annotations[\"objects\"].append({\n",
        "                    \"track_id\": track_id,\n",
        "                    \"class_id\": class_id,\n",
        "                    \"class_name\": model.names[class_id],\n",
        "                    \"confidence\": confidence,\n",
        "                    \"bbox\": {\n",
        "                        \"x1\": x1,\n",
        "                        \"y1\": y1,\n",
        "                        \"x2\": x2,\n",
        "                        \"y2\": y2\n",
        "                    }\n",
        "                })\n",
        "\n",
        "                text = f\"{track_id} - {model.names[class_id]}\"\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_DUPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "        all_annotations.append(frame_annotations)\n",
        "\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "    # After the video is processed:\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    # Save annotations JSON\n",
        "    with open(f'annotations_hyp{exp_num+1}.json', 'w') as f:\n",
        "        json.dump(all_annotations, f, indent=2)\n",
        "\n",
        "    print(f\"Saved output_hyp{exp_num+1}.mp4 and annotations_hyp{exp_num+1}.json\")\n"
      ],
      "metadata": {
        "id": "AY6qDXtzXsgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check metrics for all sets of parameters: (compered to ground truth annotations)"
      ],
      "metadata": {
        "id": "1X9a8qXZLPRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#yolo11x annotations:\n",
        "!gdown 'https://drive.google.com/uc?id=1VEjnRrq5Sxl9tYPirGeVNpNRNlieUSOQ'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsdzoQYPiNm6",
        "outputId": "63bced98-088b-4d98-a530-a9275140ec3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VEjnRrq5Sxl9tYPirGeVNpNRNlieUSOQ\n",
            "To: /content/annotations11x.json\n",
            "\r  0% 0.00/356k [00:00<?, ?B/s]\r100% 356k/356k [00:00<00:00, 125MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install motmetrics"
      ],
      "metadata": {
        "id": "XdaMZcTbL5kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "from IPython.display import Image\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import motmetrics as mm\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Function to convert JSON annotations to MOT format\n",
        "def convert_json_to_mot(json_path, output_path):\n",
        "    with open(json_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    with open(output_path, \"w\") as f_out:\n",
        "        for frame in data:\n",
        "            frame_number = frame[\"frame_number\"]\n",
        "            for obj in frame[\"objects\"]:\n",
        "                track_id = obj[\"track_id\"]\n",
        "                confidence = obj[\"confidence\"]\n",
        "                class_id = obj[\"class_id\"]\n",
        "                x1, y1 = obj[\"bbox\"][\"x1\"], obj[\"bbox\"][\"y1\"]\n",
        "                x2, y2 = obj[\"bbox\"][\"x2\"], obj[\"bbox\"][\"y2\"]\n",
        "                width, height = x2 - x1, y2 - y1\n",
        "                f_out.write(f\"{frame_number},{track_id},{x1},{y1},{width},{height},{confidence},{class_id},1\\n\")\n",
        "\n",
        "# Function to compute tracking metrics\n",
        "def compute_tracking_metrics(gt_file, pred_file):\n",
        "    acc = mm.MOTAccumulator(auto_id=True)\n",
        "    gt_data = pd.read_csv(gt_file, header=None)\n",
        "    pred_data = pd.read_csv(pred_file, header=None)\n",
        "    frames = sorted(set(gt_data[0]) | set(pred_data[0]))\n",
        "\n",
        "    for frame in frames:\n",
        "        gt_frame = gt_data[gt_data[0] == frame]\n",
        "        pred_frame = pred_data[pred_data[0] == frame]\n",
        "        gt_ids = gt_frame[1].tolist()\n",
        "        pred_ids = pred_frame[1].tolist()\n",
        "\n",
        "        def iou(boxA, boxB):\n",
        "            xA, yA, wA, hA = boxA\n",
        "            xB, yB, wB, hB = boxB\n",
        "            x1, y1 = max(xA, xB), max(yA, yB)\n",
        "            x2, y2 = min(xA + wA, xB + wB), min(yA + hA, yB + hB)\n",
        "            interArea = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "            boxAArea = wA * hA\n",
        "            boxBArea = wB * hB\n",
        "            return 1 - (interArea / float(boxAArea + boxBArea - interArea))\n",
        "\n",
        "        gt_boxes = gt_frame.iloc[:, 2:6].values.tolist()\n",
        "        pred_boxes = pred_frame.iloc[:, 2:6].values.tolist()\n",
        "        distance_matrix = [[iou(gt, pred) for pred in pred_boxes] for gt in gt_boxes]\n",
        "        acc.update(gt_ids, pred_ids, distance_matrix)\n",
        "\n",
        "    mh = mm.metrics.create()\n",
        "    summary = mh.compute(acc, metrics=['idf1', 'mota', 'motp', 'num_switches'], name=\"Tracking\")\n",
        "    return summary\n",
        "\n",
        "# LIST OF YOUR GENERATED JSONS FROM DIFFERENT HYPERPARAMETERS\n",
        "annotation_files = [\n",
        "    '/content/annotations_hyp1.json',\n",
        "    '/content/annotations_hyp2.json',\n",
        "    '/content/annotations_hyp3.json',\n",
        "    '/content/annotations_hyp4.json',\n",
        "    '/content/annotations_hyp5.json',\n",
        "    '/content/annotations_hyp6.json',\n",
        "    '/content/annotations_hyp7.json',\n",
        "    '/content/annotations_hyp8.json',\n",
        "    '/content/annotations_hyp9.json',\n",
        "    '/content/annotations_hyp10.json'\n",
        "]\n",
        "\n",
        "# Your ground truth MOT file\n",
        "convert_json_to_mot('/content/annotations11x.json', '/content/gt_mot.txt')\n",
        "gt_file=  '/content/gt_mot.txt'\n",
        "# Make a folder to save intermediate MOT prediction files\n",
        "os.makedirs('/content/mot_preds', exist_ok=True)\n",
        "\n",
        "# Loop through each annotation file\n",
        "for annotation_path in annotation_files:\n",
        "    model_name = os.path.basename(annotation_path).replace('.json', '')\n",
        "\n",
        "    pred_mot_path = f'/content/mot_preds/{model_name}_mot.txt'\n",
        "    convert_json_to_mot(annotation_path, pred_mot_path)\n",
        "\n",
        "    print(f\"\\n===== Metrics for {model_name} =====\")\n",
        "    summary = compute_tracking_metrics(gt_file, pred_mot_path)\n",
        "    print(summary)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWTmsJlyX_XG",
        "outputId": "3d989b95-7ab2-4885-be0c-41eea03fd51d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.119 ðŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 41.2/112.6 GB disk)\n",
            "\n",
            "===== Metrics for annotations_hyp0 =====\n",
            "              idf1      mota      motp  num_switches\n",
            "Tracking  0.830922  0.959337  0.138539            14\n",
            "\n",
            "\n",
            "===== Metrics for annotations_hyp1 =====\n",
            "              idf1      mota      motp  num_switches\n",
            "Tracking  0.694615  0.923946  0.159816            31\n",
            "\n",
            "\n",
            "===== Metrics for annotations_hyp2 =====\n",
            "              idf1      mota      motp  num_switches\n",
            "Tracking  0.974476  0.968373  0.134729             3\n",
            "\n",
            "\n",
            "===== Metrics for annotations_hyp3 =====\n",
            "              idf1      mota      motp  num_switches\n",
            "Tracking  0.654335  0.925452  0.153961            28\n",
            "\n",
            "\n",
            "===== Metrics for annotations_hyp4 =====\n",
            "              idf1      mota      motp  num_switches\n",
            "Tracking  0.470773  0.856175  0.214011            78\n",
            "\n",
            "\n",
            "===== Metrics for annotations_hyp5 =====\n",
            "              idf1      mota      motp  num_switches\n",
            "Tracking  0.812024  0.957078  0.141389            15\n",
            "\n",
            "\n",
            "===== Metrics for annotations_hyp6 =====\n",
            "              idf1      mota     motp  num_switches\n",
            "Tracking  0.506129  0.810241  0.23319           121\n",
            "\n",
            "\n",
            "===== Metrics for annotations_hyp7 =====\n",
            "              idf1      mota      motp  num_switches\n",
            "Tracking  0.828452  0.957078  0.140039            16\n",
            "\n",
            "\n",
            "===== Metrics for annotations_hyp8 =====\n",
            "              idf1      mota      motp  num_switches\n",
            "Tracking  0.804116  0.951054  0.142473            17\n",
            "\n",
            "\n",
            "===== Metrics for annotations_hyp9 =====\n",
            "              idf1      mota      motp  num_switches\n",
            "Tracking  0.972932  0.966114  0.133914             4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "set 3 gives best results (on this video and others).\n",
        "\n",
        "Now will try to find the specific values for the parameters given this range of numbers.\n"
      ],
      "metadata": {
        "id": "b5UYnNdYMMdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "class LocationBasedDeepSORT_Tuner:\n",
        "    def __init__(self, video_path, model_path, gt_annotations_path):\n",
        "        \"\"\"\n",
        "        Initialize the location-focused DeepSORT tuner\n",
        "\n",
        "        Args:\n",
        "            video_path: Path to input video\n",
        "            model_path: Path to YOLO detection model\n",
        "            gt_annotations_path: Path to ground truth annotations JSON\n",
        "        \"\"\"\n",
        "        self.video_path = video_path\n",
        "        self.model_path = model_path\n",
        "        self.gt_path = gt_annotations_path\n",
        "\n",
        "        # Load models and video properties once\n",
        "        self.model = YOLO(model_path)\n",
        "        self.cap = cv2.VideoCapture(video_path)\n",
        "        self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        self.frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))\n",
        "        self.cap.release()\n",
        "\n",
        "        # Metrics to optimize for (focusing on location-based metrics)\n",
        "        self.target_metrics = ['mota', 'motp', 'num_switches']\n",
        "        self.metric_weights = {\n",
        "            'mota': 0.6,       # Overall tracking accuracy\n",
        "            'motp': 0.3,       # Precision of bounding box overlap\n",
        "            'num_switches': -0.1  # Penalize ID switches (negative weight)\n",
        "        }\n",
        "\n",
        "        # Create output directories\n",
        "        os.makedirs('output_videos', exist_ok=True)\n",
        "        os.makedirs('output_annotations', exist_ok=True)\n",
        "        os.makedirs('mot_files', exist_ok=True)\n",
        "\n",
        "        # Convert ground truth to MOT format for the first 150 frames\n",
        "        self.gt_mot_path = os.path.join('mot_files', 'gt_mot_150.txt')\n",
        "        self._convert_json_to_mot(gt_annotations_path, self.gt_mot_path, max_frames=150)\n",
        "\n",
        "    def _convert_json_to_mot(self, json_path, output_path, max_frames=None):\n",
        "        \"\"\"Helper function to convert JSON annotations to MOT format for a limited number of frames\"\"\"\n",
        "        with open(json_path, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        with open(output_path, \"w\") as f_out:\n",
        "            for frame in data:\n",
        "                frame_number = frame[\"frame_number\"]\n",
        "                if max_frames is not None and frame_number > max_frames:\n",
        "                    break\n",
        "                for obj in frame[\"objects\"]:\n",
        "                    track_id = obj[\"track_id\"]\n",
        "                    confidence = obj[\"confidence\"]\n",
        "                    class_id = obj[\"class_id\"]\n",
        "                    x1, y1 = obj[\"bbox\"][\"x1\"], obj[\"bbox\"][\"y1\"]\n",
        "                    x2, y2 = obj[\"bbox\"][\"x2\"], obj[\"bbox\"][\"y2\"]\n",
        "                    width, height = x2 - x1, y2 - y1\n",
        "                    f_out.write(f\"{frame_number},{track_id},{x1},{y1},{width},{height},{confidence},{class_id},1\\n\")\n",
        "\n",
        "    def _compute_tracking_metrics(self, pred_file):\n",
        "        \"\"\"Compute tracking metrics between ground truth (first 150 frames) and predictions\"\"\"\n",
        "        acc = mm.MOTAccumulator(auto_id=True)\n",
        "        gt_data = pd.read_csv(self.gt_mot_path, header=None)\n",
        "        pred_data = pd.read_csv(pred_file, header=None)\n",
        "        gt_frames = sorted(gt_data[0].unique())\n",
        "        pred_frames = sorted(pred_data[0].unique())\n",
        "        all_frames = sorted(list(set(gt_frames) | set(pred_frames)))\n",
        "\n",
        "        for frame in all_frames:\n",
        "            if frame > 150:  # Limit evaluation to the first 150 frames\n",
        "                continue\n",
        "            gt_frame = gt_data[gt_data[0] == frame]\n",
        "            pred_frame = pred_data[pred_data[0] == frame]\n",
        "            gt_ids = gt_frame[1].tolist()\n",
        "            pred_ids = pred_frame[1].tolist()\n",
        "\n",
        "            def iou(boxA, boxB):\n",
        "                xA, yA, wA, hA = boxA\n",
        "                xB, yB, wB, hB = boxB\n",
        "                x1, y1 = max(xA, xB), max(yA, yB)\n",
        "                x2, y2 = min(xA + wA, xB + wB), min(yA + hA, yB + hB)\n",
        "                interArea = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "                boxAArea = wA * hA\n",
        "                boxBArea = wB * hB\n",
        "                return 1 - (interArea / float(boxAArea + boxBArea - interArea))\n",
        "\n",
        "            gt_boxes = gt_frame.iloc[:, 2:6].values.tolist()\n",
        "            pred_boxes = pred_frame.iloc[:, 2:6].values.tolist()\n",
        "            distance_matrix = [[iou(gt, pred) for pred in pred_boxes] for gt in gt_boxes]\n",
        "            acc.update(gt_ids, pred_ids, distance_matrix)\n",
        "\n",
        "        mh = mm.metrics.create()\n",
        "        summary = mh.compute(acc, metrics=['idf1', 'mota', 'motp', 'num_switches', 'mostly_tracked', 'mostly_lost', 'num_fragmentations'], name=\"Tracking\")\n",
        "        return summary\n",
        "\n",
        "    def _run_experiment(self, params, experiment_id):\n",
        "        \"\"\"\n",
        "        Run tracking with location-focused parameters for the first 150 frames\n",
        "\n",
        "        Args:\n",
        "            params: Dictionary of DeepSORT parameters\n",
        "            experiment_id: Unique ID for this experiment\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing metrics and output paths\n",
        "        \"\"\"\n",
        "        # Initialize paths\n",
        "        output_video_path = os.path.join('output_videos', f'output_loc{experiment_id}.mp4')\n",
        "        output_annot_path = os.path.join('output_annotations', f'annotations_loc{experiment_id}.json')\n",
        "        output_mot_path = os.path.join('mot_files', f'pred_loc{experiment_id}.txt')\n",
        "\n",
        "        # Initialize tracker with current parameters\n",
        "        tracker = DeepSort(\n",
        "            nn_budget=0,  # Disable appearance features\n",
        "            max_iou_distance=params[\"max_iou_distance\"],\n",
        "            max_cosine_distance=params[\"max_cosine_distance\"], # Effectively disable appearance matching\n",
        "            nms_max_overlap=params[\"nms_max_overlap\"]\n",
        "        )\n",
        "\n",
        "        cap = cv2.VideoCapture(self.video_path)\n",
        "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "        # Set up video writer\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "        frame_number = 0\n",
        "        all_annotations = []\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret or frame_number >= 150:\n",
        "                break\n",
        "\n",
        "            results = self.model(frame, conf=0.3, classes=0)\n",
        "\n",
        "            if results[0].boxes.data.shape[0] == 0:  # No detections in this frame\n",
        "                frame_number += 1\n",
        "                continue\n",
        "\n",
        "            frame_annotations = {\n",
        "                \"frame_number\": frame_number + 1,\n",
        "                \"objects\": []\n",
        "            }\n",
        "\n",
        "            detections = []\n",
        "            for *xyxy, conf, cls in results[0].boxes.data:\n",
        "                x1, y1, x2, y2 = map(int, xyxy)\n",
        "                detections.append([[x1, y1, x2 - x1, y2 - y1], conf.item(), int(cls.item())])\n",
        "\n",
        "            tracks = tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "            for track in tracks:\n",
        "                if not track.is_confirmed():\n",
        "                    continue\n",
        "                track_id = track.track_id\n",
        "                ltrb = track.to_ltrb()\n",
        "                class_id = track.get_det_class()\n",
        "                x1, y1, x2, y2 = map(int, ltrb)\n",
        "                confidence = track.det_conf\n",
        "\n",
        "                if confidence is not None:\n",
        "                    frame_annotations[\"objects\"].append({\n",
        "                        \"track_id\": track_id,\n",
        "                        \"class_id\": class_id,\n",
        "                        \"class_name\": self.model.names[class_id],\n",
        "                        \"confidence\": confidence,\n",
        "                        \"bbox\": {\n",
        "                            \"x1\": x1,\n",
        "                            \"y1\": y1,\n",
        "                            \"x2\": x2,\n",
        "                            \"y2\": y2\n",
        "                        }\n",
        "                    })\n",
        "\n",
        "                    text = f\"{track_id} - {self.model.names[class_id]}\"\n",
        "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                    cv2.putText(frame, text, (x1, y1 - 10), cv2.FONT_HERSHEY_DUPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "            all_annotations.append(frame_annotations)\n",
        "            out.write(frame)\n",
        "            frame_number += 1\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        # Save annotations for the first 150 frames\n",
        "        with open(output_annot_path, 'w') as f:\n",
        "            json.dump(all_annotations, f, indent=2)\n",
        "\n",
        "        # Convert predictions to MOT format for the first 150 frames\n",
        "        self._convert_json_to_mot(output_annot_path, output_mot_path, max_frames=150)\n",
        "\n",
        "        # Compute metrics\n",
        "        metrics = self._compute_tracking_metrics(output_mot_path)\n",
        "\n",
        "        return {\n",
        "            \"params\": params,\n",
        "            \"metrics\": metrics,\n",
        "            \"video_path\": output_video_path,\n",
        "            \"annotations_path\": output_annot_path,\n",
        "            \"mot_path\": output_mot_path\n",
        "        }\n",
        "\n",
        "    def _calculate_score(self, metrics_df):\n",
        "        \"\"\"Calculate a weighted score based on location-focused metrics\"\"\"\n",
        "        score = 0\n",
        "        for metric, weight in self.metric_weights.items():\n",
        "            if metric in ['mota', 'motp']:\n",
        "                # Higher is better\n",
        "                score += metrics_df[metric].iloc[0] * weight\n",
        "            elif metric in ['num_switches']:\n",
        "                # Lower is better, so we subtract\n",
        "                max_val = max(metrics_df[metric].max(), 1)  # Avoid division by zero\n",
        "                normalized = metrics_df[metric].iloc[0] / max_val\n",
        "                score += (1 - normalized) * abs(weight)  # Use absolute value since weight is negative\n",
        "        return score\n",
        "\n",
        "    def optimize_location_params(self, num_iterations=10):\n",
        "        \"\"\"\n",
        "        Optimize location-focused parameters using a more controlled search with a step of 0.05\n",
        "\n",
        "        Args:\n",
        "            num_iterations: Number of optimization iterations (will be less if all combinations are tested)\n",
        "\n",
        "        Returns:\n",
        "            Best parameters found and full history of evaluations\n",
        "        \"\"\"\n",
        "        best_score = -np.inf\n",
        "        best_params = None\n",
        "        history = []\n",
        "        tested_params = set()\n",
        "\n",
        "        param_ranges = {\n",
        "            \"max_iou_distance\": np.arange(0.6, 0.7, 0.05),\n",
        "            \"nms_max_overlap\": np.arange(0.5, 0.7, 0.05),\n",
        "            \"max_cosine_distance\": np.arange(0.15, 0.3, 0.05)\n",
        "        }\n",
        "\n",
        "        # Generate all combinations of parameters\n",
        "        param_combinations = list(ParameterGrid(param_ranges))\n",
        "        total_combinations = len(param_combinations)\n",
        "        print(f\"Total parameter combinations to test: {total_combinations}\")\n",
        "\n",
        "        for i, params in enumerate(param_combinations):\n",
        "            params_tuple = tuple(sorted(params.items())) # For checking if params were already tested\n",
        "            if params_tuple in tested_params:\n",
        "                continue\n",
        "            tested_params.add(params_tuple)\n",
        "\n",
        "            experiment_result = self._run_experiment(params, i)\n",
        "            score = self._calculate_score(experiment_result[\"metrics\"])\n",
        "\n",
        "            history.append({\n",
        "                \"params\": params,\n",
        "                \"score\": score,\n",
        "                \"mota\": experiment_result[\"metrics\"]['mota'].iloc[0],\n",
        "                \"motp\": experiment_result[\"metrics\"]['motp'].iloc[0],\n",
        "                \"num_switches\": experiment_result[\"metrics\"]['num_switches'].iloc[0]\n",
        "            })\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = params.copy()\n",
        "                print(f\"New best score: {best_score:.4f} with params: {best_params}\")\n",
        "\n",
        "        history_df = pd.DataFrame(history).sort_values('score', ascending=False)\n",
        "        return best_params, history_df\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize tuner with your paths\n",
        "    tuner = LocationBasedDeepSORT_Tuner(\n",
        "        video_path='/content/birds-flying-in-blue-sky-preview.mp4',\n",
        "        model_path='/content/bestbirdsonly.pt',\n",
        "        gt_annotations_path='/content/annotations11x.json'\n",
        "    )\n",
        "\n",
        "    # Run optimization\n",
        "    best_params, history = tuner.optimize_location_params(num_iterations=None) # Will test all combinations\n",
        "\n",
        "\n",
        "    # Save full history\n",
        "    history.to_csv('location_based_optimization_history_150frames.csv', index=False)"
      ],
      "metadata": {
        "id": "BMQdZam5Mdqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBest parameters found for location-based tracking:\")\n",
        "print(best_params)\n",
        "\n",
        "print(\"\\nTop 5 configurations:\")\n",
        "print(history.to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G52vVwJVJDMu",
        "outputId": "f2b8b8fe-415f-404f-96b4-82e2683dd210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best parameters found for location-based tracking:\n",
            "{'max_cosine_distance': np.float64(0.2), 'max_iou_distance': np.float64(0.65), 'nms_max_overlap': np.float64(0.5)}\n",
            "\n",
            "Top 5 configurations:\n",
            "                                                                                            params     score      mota      motp  num_switches\n",
            "21                {'max_cosine_distance': 0.25, 'max_iou_distance': 0.65, 'nms_max_overlap': 0.55}  0.625984  0.975452  0.135709             1\n",
            "12                  {'max_cosine_distance': 0.2, 'max_iou_distance': 0.65, 'nms_max_overlap': 0.5}  0.625984  0.975452  0.135709             1\n",
            "20                 {'max_cosine_distance': 0.25, 'max_iou_distance': 0.65, 'nms_max_overlap': 0.5}  0.625984  0.975452  0.135709             1\n",
            "22  {'max_cosine_distance': 0.25, 'max_iou_distance': 0.65, 'nms_max_overlap': 0.6000000000000001}  0.625984  0.975452  0.135709             1\n",
            "14   {'max_cosine_distance': 0.2, 'max_iou_distance': 0.65, 'nms_max_overlap': 0.6000000000000001}  0.625984  0.975452  0.135709             1\n",
            "13                 {'max_cosine_distance': 0.2, 'max_iou_distance': 0.65, 'nms_max_overlap': 0.55}  0.625984  0.975452  0.135709             1\n",
            "8                    {'max_cosine_distance': 0.2, 'max_iou_distance': 0.6, 'nms_max_overlap': 0.5}  0.625063  0.974160  0.135224             1\n",
            "9                   {'max_cosine_distance': 0.2, 'max_iou_distance': 0.6, 'nms_max_overlap': 0.55}  0.625063  0.974160  0.135224             1\n",
            "17                 {'max_cosine_distance': 0.25, 'max_iou_distance': 0.6, 'nms_max_overlap': 0.55}  0.625063  0.974160  0.135224             1\n",
            "18   {'max_cosine_distance': 0.25, 'max_iou_distance': 0.6, 'nms_max_overlap': 0.6000000000000001}  0.625063  0.974160  0.135224             1\n",
            "10    {'max_cosine_distance': 0.2, 'max_iou_distance': 0.6, 'nms_max_overlap': 0.6000000000000001}  0.625063  0.974160  0.135224             1\n",
            "16                  {'max_cosine_distance': 0.25, 'max_iou_distance': 0.6, 'nms_max_overlap': 0.5}  0.625063  0.974160  0.135224             1\n",
            "23  {'max_cosine_distance': 0.25, 'max_iou_distance': 0.65, 'nms_max_overlap': 0.6500000000000001}  0.624505  0.972868  0.135948             2\n",
            "15   {'max_cosine_distance': 0.2, 'max_iou_distance': 0.65, 'nms_max_overlap': 0.6500000000000001}  0.624505  0.972868  0.135948             2\n",
            "6   {'max_cosine_distance': 0.15, 'max_iou_distance': 0.65, 'nms_max_overlap': 0.6000000000000001}  0.623923  0.972868  0.134005             1\n",
            "5                 {'max_cosine_distance': 0.15, 'max_iou_distance': 0.65, 'nms_max_overlap': 0.55}  0.623923  0.972868  0.134005             1\n",
            "4                  {'max_cosine_distance': 0.15, 'max_iou_distance': 0.65, 'nms_max_overlap': 0.5}  0.623923  0.972868  0.134005             1\n",
            "11    {'max_cosine_distance': 0.2, 'max_iou_distance': 0.6, 'nms_max_overlap': 0.6500000000000001}  0.623585  0.971576  0.135463             2\n",
            "19   {'max_cosine_distance': 0.25, 'max_iou_distance': 0.6, 'nms_max_overlap': 0.6500000000000001}  0.623585  0.971576  0.135463             2\n",
            "0                   {'max_cosine_distance': 0.15, 'max_iou_distance': 0.6, 'nms_max_overlap': 0.5}  0.623171  0.971576  0.134083             1\n",
            "2    {'max_cosine_distance': 0.15, 'max_iou_distance': 0.6, 'nms_max_overlap': 0.6000000000000001}  0.623171  0.971576  0.134083             1\n",
            "1                  {'max_cosine_distance': 0.15, 'max_iou_distance': 0.6, 'nms_max_overlap': 0.55}  0.623171  0.971576  0.134083             1\n",
            "7   {'max_cosine_distance': 0.15, 'max_iou_distance': 0.65, 'nms_max_overlap': 0.6500000000000001}  0.622585  0.970284  0.134714             2\n",
            "3    {'max_cosine_distance': 0.15, 'max_iou_distance': 0.6, 'nms_max_overlap': 0.6500000000000001}  0.621706  0.968992  0.134368             2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Best parameters we got:**\n",
        "\n",
        "max_cosine_distance: 0.25\n",
        "\n",
        "max_iou_distance': 0.65\n",
        "\n",
        "nms_max_overlap': 0.5"
      ],
      "metadata": {
        "id": "ZQaLDgP2PbsF"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}